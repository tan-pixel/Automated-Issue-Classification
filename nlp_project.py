# -*- coding: utf-8 -*-
"""nlp-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F7XSr-LPmJ-MA7OiUbr_iFqmmrjlbYrm

## Import Libraries
"""

!pip install transformers

!pip install -U sentence-transformers

!pip install xgboost

#Imports required packages
import pandas as pd
import numpy as np
import sklearn
from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score
from sklearn.model_selection import GridSearchCV, train_test_split

from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from lightgbm import LGBMRegressor

"""## Dataset Download"""

!wget https://raw.githubusercontent.com/nlbse2024/issue-report-classification/main/data/issues_train.csv
!wget https://raw.githubusercontent.com/nlbse2024/issue-report-classification/main/data/issues_test.csv

# loading training data
traindf = pd.read_csv('issues_train.csv')

# loading testing data
testdf = pd.read_csv('issues_test.csv')

traindf

testdf

"""## Dataset Preprocessing"""

traindf.drop(['repo','created_at'],axis=1,inplace=True)
testdf.drop(['repo','created_at'],axis=1,inplace=True)

# checking the shape of the data
traindf.shape,testdf.shape

print(traindf.isnull().sum())
print(testdf.isnull().sum())

traindf.dropna(inplace = True)
testdf.dropna(inplace = True)

testdf.isnull().sum()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
traindf['label'] = le.fit_transform(traindf['label'])
print(traindf.label.unique())
testdf['label'] = le.transform(testdf['label'])
print(testdf.label.unique())

import re

def preprocess(issues):
    processed_issues = []

    for issue in issues:

        if not isinstance(issue, str):
            issue = str(issue) if issue is not None else ''

        # Remove strings between triple quotes
        issue = re.sub(r'```.*?```', ' ', issue, flags=re.DOTALL)

        # Remove new lines
        issue = re.sub(r'\n', ' ', issue)

        # Remove links
        issue = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', issue)

        # Remove digits
        issue = re.sub(r'\d+', ' ', issue)

        # Remove special characters except the question marks
        issue = re.sub(r'[^a-zA-Z0-9?\s]', ' ', issue)
        issue = re.sub(r'\s+', ' ', issue)

        processed_issues.append(issue)

    return processed_issues

traindf['title'] = preprocess(traindf['title'])
traindf['body'] = preprocess(traindf['body'])

testdf['title'] = preprocess(testdf['title'])
testdf['body'] = preprocess(testdf['body'])

# Saving data to csv files
traindf.to_csv('github_issue_classification_train.csv')
testdf.to_csv('github_issue_classification_test1.csv')

traindf = pd.read_csv('/kaggle/input/issue-classification/github_issue_classification_train.csv')
testdf = pd.read_csv('/kaggle/input/issue-classification/github_issue_classification_test.csv')

"""# Generating Embeddings

## Body embeddings

### Sentence Transformer
"""

from sentence_transformers import SentenceTransformer
import torch
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader

## body
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load https://huggingface.co/sentence-transformers/all-mpnet-base-v2
model1 = SentenceTransformer("all-mpnet-base-v2")

# Batch size
batch_size = 16  # Adjust according to your GPU memory

# Function to create a DataLoader that splits the text data into batches
def create_dataloader(text_data, batch_size):
    return DataLoader(text_data, batch_size=batch_size, shuffle=False)

# Create DataLoader for both train and test body columns
train_dataloader = create_dataloader(traindf["body"].values.tolist(), batch_size)
test_dataloader = create_dataloader(testdf["body"].values.tolist(), batch_size)

# Function to process each batch
def process_batches(dataloader):
    all_cls_embeddings = []

    for batch in dataloader:
        # # Tokenize the batch
        # tokenized_batch = tokenizer1(list(batch), padding=True, truncation=True, return_tensors="pt")

        # # Move tokenized data to device (GPU/CPU)
        # tokenized_batch = {k: v.to(device) for k, v in tokenized_batch.items()}

        # Get hidden states without gradient calculation
        with torch.no_grad():
            hidden_states = model1.encode(batch)
            # print(type(hidden_states))

        # Extract [CLS] token's hidden state for each sentence in the batch
        cls_embeddings = hidden_states[:, :]  # Shape: [batch_size, emb_dim]
        all_cls_embeddings.append(cls_embeddings)

    # Concatenate all the embeddings for all batches
    all_cls_embeddings = [torch.from_numpy(embedding) if isinstance(embedding, np.ndarray) else embedding for embedding in all_cls_embeddings]
    all_cls_embeddings = torch.cat(all_cls_embeddings, dim=0)

    return all_cls_embeddings

# Process the batches for train and test datasets
cls_train_body_sent = process_batches(train_dataloader)
cls_test_body_sent = process_batches(test_dataloader)

"""### Roberta-base"""

import torch
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the tokenizer and model for RoBERTa
tokenizer_roberta = AutoTokenizer.from_pretrained("FacebookAI/roberta-base")
model_roberta = AutoModel.from_pretrained("FacebookAI/roberta-base").to(device)

# Set batch size
batch_size = 16  # You can adjust this based on your memory constraints

# Create DataLoader for batching the input text
def create_dataloader(text_data, batch_size):
    return DataLoader(text_data, batch_size=batch_size, shuffle=False)

# Create DataLoader for the train and test titles
train_dataloader_body = create_dataloader(traindf["body"].values.tolist(), batch_size)
test_dataloader_body = create_dataloader(testdf["body"].values.tolist(), batch_size)

# Function to process each batch and extract [CLS] token embeddings
def process_batches_roberta(dataloader):
    all_cls_embeddings = []

    for batch in dataloader:
        # Tokenize the batch
        tokenized_batch = tokenizer_roberta(list(batch), padding=True, truncation=True, return_tensors="pt")

        # Move tokenized data to the appropriate device (GPU/CPU)
        tokenized_batch = {k: v.to(device) for k, v in tokenized_batch.items()}

        # Forward pass to get the hidden states without gradient calculation
        with torch.no_grad():
            hidden_states = model_roberta(**tokenized_batch)

        # Extract [CLS] token's hidden state for each sentence in the batch
        cls_embeddings = hidden_states.last_hidden_state[:, 0, :]  # Shape: [batch_size, emb_dim]
        all_cls_embeddings.append(cls_embeddings)

    # Concatenate all the [CLS] embeddings for all batches
    all_cls_embeddings = torch.cat(all_cls_embeddings, dim=0)

    return all_cls_embeddings

# Process batches for both train and test datasets
cls_train_body_rob = process_batches_roberta(train_dataloader_body)
cls_test_body_rob = process_batches_roberta(test_dataloader_body)

"""## Title Embeddings

### Sentence Transformer
"""

## title
from sentence_transformers import SentenceTransformer
import torch
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model2 = SentenceTransformer("all-mpnet-base-v2")

# Set batch size
batch_size = 16  # You can adjust this based on your memory constraints

# Create DataLoader for batching the input text
def create_dataloader(text_data, batch_size):
    return DataLoader(text_data, batch_size=batch_size, shuffle=False)

# Create DataLoader for the train and test titles
train_dataloader_title = create_dataloader(traindf["title"].values.tolist(), batch_size)
test_dataloader_title = create_dataloader(testdf["title"].values.tolist(), batch_size)

# Function to process each batch and extract [CLS] token embeddings
def process_batches_roberta(dataloader):
    all_cls_embeddings = []

    for batch in dataloader:
        # # Tokenize the batch
        # tokenized_batch = tokenizer2(list(batch), padding=True, truncation=True, return_tensors="pt")

        # # Move tokenized data to the appropriate device (GPU/CPU)
        # tokenized_batch = {k: v.to(device) for k, v in tokenized_batch.items()}

        # Forward pass to get the hidden states without gradient calculation
        with torch.no_grad():
            hidden_states = model2.encode(batch)

        # Extract [CLS] token's hidden state for each sentence in the batch
        cls_embeddings = hidden_states[:, :]  # Shape: [batch_size, emb_dim]
        all_cls_embeddings.append(cls_embeddings)

    # Concatenate all the [CLS] embeddings for all batches
    all_cls_embeddings = [torch.from_numpy(embedding) if isinstance(embedding, np.ndarray) else embedding for embedding in all_cls_embeddings]
    all_cls_embeddings = torch.cat(all_cls_embeddings, dim=0)

    return all_cls_embeddings

# Process batches for both train and test datasets
cls_train_title_sent = process_batches_roberta(train_dataloader_title)
cls_test_title_sent = process_batches_roberta(test_dataloader_title)

"""### Roberta-base"""

import torch
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the tokenizer and model for RoBERTa
tokenizer_roberta = AutoTokenizer.from_pretrained("FacebookAI/roberta-base")
model_roberta = AutoModel.from_pretrained("FacebookAI/roberta-base").to(device)

# Set batch size
batch_size = 16  # You can adjust this based on your memory constraints

# Create DataLoader for batching the input text
def create_dataloader(text_data, batch_size):
    return DataLoader(text_data, batch_size=batch_size, shuffle=False)

# Create DataLoader for the train and test titles
train_dataloader_title = create_dataloader(traindf["title"].values.tolist(), batch_size)
test_dataloader_title = create_dataloader(testdf["title"].values.tolist(), batch_size)

# Function to process each batch and extract [CLS] token embeddings
def process_batches_roberta(dataloader):
    all_cls_embeddings = []

    for batch in dataloader:
        # Tokenize the batch
        tokenized_batch = tokenizer_roberta(list(batch), padding=True, truncation=True, return_tensors="pt")

        # Move tokenized data to the appropriate device (GPU/CPU)
        tokenized_batch = {k: v.to(device) for k, v in tokenized_batch.items()}

        # Forward pass to get the hidden states without gradient calculation
        with torch.no_grad():
            hidden_states = model_roberta(**tokenized_batch)

        # Extract [CLS] token's hidden state for each sentence in the batch
        cls_embeddings = hidden_states.last_hidden_state[:, 0, :]  # Shape: [batch_size, emb_dim]
        all_cls_embeddings.append(cls_embeddings)

    # Concatenate all the [CLS] embeddings for all batches
    all_cls_embeddings = torch.cat(all_cls_embeddings, dim=0)

    return all_cls_embeddings

# Process batches for both train and test datasets
cls_train_title_rob = process_batches_roberta(train_dataloader_title)
cls_test_title_rob = process_batches_roberta(test_dataloader_title)

"""# Features Processing"""

emb_array_train  = []
emb_array_test = []

emb_sent_sent_train = np.concatenate((cls_train_body_sent.detach().cpu().numpy(), cls_train_title_sent.detach().cpu().numpy()), axis=1)
emb_sent_sent_test = np.concatenate((cls_test_body_sent.detach().cpu().numpy(), cls_test_title_sent.detach().cpu().numpy()), axis=1)

emb_sent_rob_train = np.concatenate((cls_train_body_sent.detach().cpu().numpy(), cls_train_title_rob.detach().cpu().numpy()), axis=1)
emb_sent_rob_test = np.concatenate((cls_test_body_sent.detach().cpu().numpy(), cls_test_title_rob.detach().cpu().numpy()), axis=1)

emb_rob_sent_train = np.concatenate((cls_train_body_rob.detach().cpu().numpy(), cls_train_title_sent.detach().cpu().numpy()), axis=1)
emb_rob_sent_test = np.concatenate((cls_test_body_rob.detach().cpu().numpy(), cls_test_title_sent.detach().cpu().numpy()), axis=1)

emb_rob_rob_train = np.concatenate((cls_train_body_rob.detach().cpu().numpy(), cls_train_title_rob.detach().cpu().numpy()), axis=1)
emb_rob_rob_test = np.concatenate((cls_test_body_rob.detach().cpu().numpy(), cls_test_title_rob.detach().cpu().numpy()), axis=1)

scaler = MinMaxScaler()

emb_array_train.append(scaler.fit_transform(emb_sent_sent_train))
emb_array_test.append(scaler.transform(emb_sent_sent_test))

emb_array_train.append(scaler.fit_transform(emb_sent_rob_train))
emb_array_test.append(scaler.transform(emb_sent_rob_test))

emb_array_train.append(scaler.fit_transform(emb_rob_sent_train))
emb_array_test.append(scaler.transform(emb_rob_sent_test))

emb_array_train.append(scaler.fit_transform(emb_rob_rob_train))
emb_array_test.append(scaler.transform(emb_rob_rob_test))

emb_array_train = np.array(emb_array_train)
emb_array_test = np.array(emb_array_test)

x_train = emb_array_train
y_train = np.array([traindf["label"], traindf["label"], traindf["label"], traindf["label"]])
x_test = emb_array_test
y_test = np.array([testdf["label"], testdf["label"], testdf["label"], testdf["label"]])

# x_val = cls_val.to("cpu")
# y_val = df_val["label"]

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

import pickle

with open("x_train.pkl", "wb") as f:
    pickle.dump(x_train, f)
with open("y_train.pkl", "wb") as f:
    pickle.dump(y_train, f)
with open("x_test.pkl", "wb") as f:
    pickle.dump(x_test, f)
with open("y_test.pkl", "wb") as f:
    pickle.dump(y_test, f)

"""# LSTM"""

import pickle
with open("x_train.pkl", "rb") as f:
      x_train = pickle.load(f)
with open("y_train.pkl", "rb") as f:
    y_train = pickle.load(f)
with open("x_test.pkl", "rb") as f:
    x_test = pickle.load(f)
with open("y_test.pkl", "rb") as f:
    y_test = pickle.load(f)

!pip install ray

import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from ray import tune
from ray.tune.schedulers import ASHAScheduler

# import torch
# import torch.nn as nn

# # LSTM Classifier with fixed hidden dimensions (256 for first layer, 128 for second layer)
# def reset_lstm(input_dim, num_classes, dropout, device):
#     class LSTMClassifier(nn.Module):
#         def __init__(self, input_dim, num_classes, dropout):
#             super(LSTMClassifier, self).__init__()

#             # Define LSTM layers with fixed hidden dimensions (256 for first layer, 128 for second layer)
#             self.lstm1 = nn.LSTM(input_dim, 32, num_layers=2, batch_first=True, dropout=dropout)
#             self.lstm2 = nn.LSTM(32, 16, num_layers=2, batch_first=True, dropout=dropout)

#             # Fully connected layer
#             self.fc = nn.Linear(16, num_classes)  # Using 128 as the input to the fc layer

#         def forward(self, x):
#             # x: [batch_size, seq_len, input_dim]

#             # Pass input through the first LSTM layer
#             out1, (hn1, _) = self.lstm1(x)  # out1: [batch_size, seq_len, 256], hn1: [2, batch_size, 256]

#             # Pass the output of the first LSTM layer through the second LSTM layer
#             out2, (hn2, _) = self.lstm2(out1)  # out2: [batch_size, seq_len, 128], hn2: [2, batch_size, 128]

#             # Get the last hidden state of the second LSTM layer (hn2 is of shape [2, batch_size, 128])
#             x = hn2[-1]  # Shape: [batch_size, 128]

#             # Pass through the fully connected layer
#             x = self.fc(x)  # Output: [batch_size, num_classes]
#             return x

#     # Create and return the model, moving it to the specified device (GPU/CPU)
#     model = LSTMClassifier(input_dim, num_classes, dropout)
#     return model.to(device)

import torch
import torch.nn as nn

# LSTM Classifier with fixed hidden dimensions (256 for first layer, 128 for second layer)
def reset_lstm(input_dim, num_classes, dropout, device):
    class LSTMClassifier(nn.Module):
        def __init__(self, input_dim, num_classes, dropout):
            super(LSTMClassifier, self).__init__()

            # Define LSTM layers with fixed hidden dimensions (256 for first layer, 128 for second layer)
            self.lstm1 = nn.LSTM(input_dim, 64, num_layers=2, batch_first=True, dropout=dropout)

            # Fully connected layer
            self.fc = nn.Linear(64, num_classes)  # Using 128 as the input to the fc layer

        def forward(self, x):
            # x: [batch_size, seq_len, input_dim]

            # Pass input through the first LSTM layer
            out1, (hn1, _) = self.lstm1(x)  # out1: [batch_size, seq_len, 256], hn1: [2, batch_size, 256]

            # Get the last hidden state of the second LSTM layer (hn2 is of shape [2, batch_size, 128])
            x = hn1[-1]  # Shape: [batch_size, 128]

            # Pass through the fully connected layer
            x = self.fc(x)  # Output: [batch_size, num_classes]
            return x

    # Create and return the model, moving it to the specified device (GPU/CPU)
    model = LSTMClassifier(input_dim, num_classes, dropout)
    return model.to(device)

class EmbeddingDataset(Dataset):
    def __init__(self, embeddings, labels):
        self.embeddings = embeddings
        self.labels = labels

    def __len__(self):
        return len(self.embeddings)

    def __getitem__(self, idx):
        return {
            "embedding": torch.tensor(self.embeddings[idx], dtype=torch.float),
            "label": torch.tensor(self.labels[idx], dtype=torch.long),
        }

for i in range(4):
    # Use all of x_train and y_train for training
    train_embeddings = x_train[i]
    train_labels = y_train[i]

    # Use x_test and y_test for evaluation
    test_embeddings = x_test[i]
    test_labels = y_test[i]

    # Create datasets
    train_dataset = EmbeddingDataset(train_embeddings, train_labels)
    test_dataset = EmbeddingDataset(test_embeddings, test_labels)

    # DataLoaders for training and test datasets
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Initialize and train the final model
    final_model = reset_lstm(
        input_dim=1536,
        num_classes=3,
        dropout=0.2,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    optimizer = torch.optim.Adam(final_model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(100):
        final_model.train()
        for batch in train_loader:
            embeddings = batch["embedding"].unsqueeze(1).to(device)  # [batch_size, 1, input_dim]
            labels = batch["label"].to(device)  # [batch_size]

            optimizer.zero_grad()
            outputs = final_model(embeddings)  # [batch_size, num_classes]

            # Ensure the output shape is [batch_size, num_classes]
            loss = criterion(outputs.view(embeddings.shape[0], 3), labels)  # CrossEntropyLoss expects outputs of shape [batch_size, num_classes] and labels of shape [batch_size]
            loss.backward()
            optimizer.step()

    # Evaluate on test set
    final_model.eval()
    test_preds, test_labels = [], []
    with torch.no_grad():
        for batch in test_loader:
            embeddings = batch["embedding"].unsqueeze(1).to(device)  # [batch_size, 1, input_dim]
            labels = batch["label"].to(device)  # [batch_size]
            outputs = final_model(embeddings)
            preds = torch.argmax(outputs, dim=1)
            test_preds.extend(preds.cpu().numpy())
            test_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(test_labels, test_preds)
    f1 = f1_score(test_labels, test_preds, average="weighted")
    precision = precision_score(test_labels, test_preds, average="weighted")
    recall = recall_score(test_labels, test_preds, average="weighted")

    print(f"Combination {i+1} Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")

"""# DistilBERT + Adapters"""

import pickle
with open("x_train.pkl", "rb") as f:
    x_train = pickle.load(f)
with open("y_train.pkl", "rb") as f:
    y_train = pickle.load(f)
with open("x_test.pkl", "rb") as f:
    x_test = pickle.load(f)
with open("y_test.pkl", "rb") as f:
    y_test = pickle.load(f)

pip install adapters

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import DistilBertModel, DistilBertConfig
from adapters import AdapterConfig, DistilBertAdapterModel
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import torch.nn as nn

# Example Dataset class
class EmbeddingDataset(Dataset):
    def __init__(self, embeddings, labels):
        self.embeddings = embeddings
        self.labels = labels

    def __len__(self):
        return len(self.embeddings)

    def __getitem__(self, idx):
        return {
            "embedding": torch.tensor(self.embeddings[idx], dtype=torch.float),
            "label": torch.tensor(self.labels[idx], dtype=torch.long),
        }

# class ProjectionLayer(nn.Module):
#     def __init__(self, input_dim, output_dim):
#         super(ProjectionLayer, self).__init__()
#         self.fc = nn.Linear(input_dim, output_dim)

#     def forward(self, x):
#         return self.fc(x)

# projection = ProjectionLayer(input_dim=1536, output_dim=768).to(device)

# from transformers import BertConfig, BertModel

# config = BertConfig(hidden_size=768, num_hidden_layers=1, num_attention_heads=8)
# transformer = BertModel(config)

# concatenated_embeddings = torch.cat((embedding1.unsqueeze(1), embedding2.unsqueeze(1)), dim=1)  # Shape: [batch_size, 2, 768]
# transformer_output = transformer(inputs_embeds=concatenated_embeddings)
# reduced_embeddings = transformer_output.last_hidden_state.mean(dim=1)  # Take the mean across sequence dimension

for i in range(len(x_train)):
  print(f"Training model for combination {i+1}")

  # Prepare datasets and dataloaders
  train_dataset = EmbeddingDataset(x_train[i], y_train[i])
  test_dataset = EmbeddingDataset(x_test[i], y_test[i])

  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

  # Load DistilBert with Adapters
  config = DistilBertConfig.from_pretrained("distilbert-base-uncased")
  model = DistilBertAdapterModel.from_pretrained("distilbert-base-uncased", config=config)

  # Add adapter
  adapter_config = AdapterConfig.load("pfeiffer")
  model.add_adapter("classification_adapter", config=adapter_config)
  model.train_adapter("classification_adapter")

  # Add classification head
  model.add_classification_head(
      "classification_adapter", num_labels=3, id2label={0: "class_0", 1: "class_1", 2: "class_2"}
  )

  # Define optimizer and criterion
#   optimizer = torch.optim.AdamW(
#     list(model.parameters()) + list(projection.parameters()),
#     lr=1e-3
# )

  optimizer = torch.optim.AdamW(
    list(model.parameters()),
    lr=1e-3)

  criterion = torch.nn.CrossEntropyLoss()

  # Training loop
  model.train()
  # projection.train()  # Ensure projection layer is in training mode
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  for epoch in range(200):  # Fixed number of epochs for simplicity
      total_loss = 0
      for batch in train_loader:
          embeddings = batch["embedding"].view(batch["embedding"].shape[0], 2, 768).to(device)  # Shape: [batch_size, hidden_dim]
          labels = batch["label"].to(device)         # Shape: [batch_size]

          optimizer.zero_grad()
          outputs = model(inputs_embeds=embeddings, adapter_name="classification_adapter")
          logits = outputs.logits.squeeze(1)  # Remove the sequence length dimension
          loss = criterion(logits, labels)
          loss.backward()
          optimizer.step()
          total_loss += loss.item()

      print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")

  # Evaluation
  model.eval()
  preds, true_labels = [], []

  with torch.no_grad():
      for batch in test_loader:
          embeddings = batch["embedding"].view(batch["embedding"].shape[0], 2, 768).to(device)  # Shape: [batch_size, hidden_dim]
          labels = batch["label"].to(device)         # Shape: [batch_size]

          # Add sequence length dimension and forward pass
          outputs = model(inputs_embeds=embeddings, adapter_name="classification_adapter")
          logits = outputs.logits.squeeze(1)  # Shape: [batch_size, num_labels]

          # Predictions
          predictions = torch.argmax(logits, dim=1)  # Shape: [batch_size]

          # # Debugging (ensure shapes match)
          # print(f"Batch size (labels): {labels.size()}, Batch size (predictions): {predictions.size()}")

          # Accumulate predictions and true labels
          preds.extend(predictions.cpu().numpy())
          true_labels.extend(labels.cpu().numpy())

  # # Check for mismatches before computing metrics
  # print(f"Total samples in preds: {len(preds)}, Total samples in true_labels: {len(true_labels)}")

  if len(preds) == len(true_labels):
      accuracy = accuracy_score(true_labels, preds)
      f1 = f1_score(true_labels, preds, average="weighted")
      precision = precision_score(true_labels, preds, average="weighted")
      recall = recall_score(true_labels, preds, average="weighted")
      print(f"\n-----------------------------------------------------------")
      print(f"Accuracy: {accuracy:.4f}")
      print(f"F1 Score: {f1:.4f}")
      print(f"Precision: {precision:.4f}")
      print(f"Recall: {recall:.4f}")
      print(f"-----------------------------------------------------------")
  else:
      print("Mismatch in number of predictions and true labels!")

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import RobertaConfig
from adapters import RobertaAdapterModel
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import torch.nn as nn

# Example Dataset class
class EmbeddingDataset(Dataset):
    def __init__(self, embeddings, labels):
        self.embeddings = embeddings
        self.labels = labels

    def __len__(self):
        return len(self.embeddings)

    def __getitem__(self, idx):
        return {
            "embedding": torch.tensor(self.embeddings[idx], dtype=torch.float),
            "label": torch.tensor(self.labels[idx], dtype=torch.long),
        }

class ProjectionLayer(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(ProjectionLayer, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.fc(x)

projection = ProjectionLayer(input_dim=1536, output_dim=768).to(device)

for i in range(len(x_train)):
  print(f"Training model for combination {i+1}")

  # Prepare datasets and dataloaders
  train_dataset = EmbeddingDataset(x_train[i], y_train[i])
  test_dataset = EmbeddingDataset(x_test[i], y_test[i])

  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

  # Load Roberta with Adapters
  config = RobertaConfig.from_pretrained("roberta-base")
  model = RobertaAdapterModel.from_pretrained("roberta-base", config=config)

  # Add adapter
  adapter_config = AdapterConfig.load("pfeiffer")
  model.add_adapter("classification_adapter", config=adapter_config)
  model.train_adapter("classification_adapter")

  # Add classification head
  model.add_classification_head(
      "classification_adapter", num_labels=3, id2label={0: "class_0", 1: "class_1", 2: "class_2"}
  )

  # Define optimizer and criterion
#   optimizer = torch.optim.AdamW(
#     list(model.parameters()) + list(projection.parameters()),
#     lr=1e-3
# )

  optimizer = torch.optim.AdamW(
    list(model.parameters()),
    lr=1e-3)

  criterion = torch.nn.CrossEntropyLoss()

  # Training loop
  model.train()
  # projection.train()  # Ensure projection layer is in training mode
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  for epoch in range(100):  # Fixed number of epochs for simplicity
      total_loss = 0
      for batch in train_loader:
          embeddings = batch["embedding"].view(batch["embedding"].shape[0], 2, 768).to(device)  # Shape: [batch_size, hidden_dim]
          labels = batch["label"].to(device)         # Shape: [batch_size]

          optimizer.zero_grad()
          outputs = model(inputs_embeds=embeddings, adapter_name="classification_adapter")
          logits = outputs.logits.squeeze(1)  # Remove the sequence length dimension
          loss = criterion(logits, labels)
          loss.backward()
          optimizer.step()
          total_loss += loss.item()

      print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")

  # Evaluation
  model.eval()
  preds, true_labels = [], []

  with torch.no_grad():
      for batch in test_loader:
          embeddings = batch["embedding"].view(batch["embedding"].shape[0], 2, 768).to(device)  # Shape: [batch_size, hidden_dim]
          labels = batch["label"].to(device)         # Shape: [batch_size]

          # Add sequence length dimension and forward pass
          outputs = model(inputs_embeds=embeddings, adapter_name="classification_adapter")
          logits = outputs.logits.squeeze(1)  # Shape: [batch_size, num_labels]

          # Predictions
          predictions = torch.argmax(logits, dim=1)  # Shape: [batch_size]

          # # Debugging (ensure shapes match)
          # print(f"Batch size (labels): {labels.size()}, Batch size (predictions): {predictions.size()}")

          # Accumulate predictions and true labels
          preds.extend(predictions.cpu().numpy())
          true_labels.extend(labels.cpu().numpy())

  # # Check for mismatches before computing metrics
  # print(f"Total samples in preds: {len(preds)}, Total samples in true_labels: {len(true_labels)}")

  if len(preds) == len(true_labels):
      accuracy = accuracy_score(true_labels, preds)
      f1 = f1_score(true_labels, preds, average="weighted")
      precision = precision_score(true_labels, preds, average="weighted")
      recall = recall_score(true_labels, preds, average="weighted")
      print(f"\n-----------------------------------------------------------")
      print(f"Accuracy: {accuracy:.4f}")
      print(f"F1 Score: {f1:.4f}")
      print(f"Precision: {precision:.4f}")
      print(f"Recall: {recall:.4f}")
      print(f"-----------------------------------------------------------")
  else:
      print("Mismatch in number of predictions and true labels!")

"""## SVM"""

for i in range(x_train):
  svc = SVC(C=1.6)
  svc.fit(x_train[i], y_train[i])
  y_preds = svc.predict(x_test[i])

  print(accuracy_score(y_test[i], y_preds))
  print(f1_score(y_test[i], y_preds, average='micro'))
  print(precision_score(y_test[i], y_preds, average='micro'))
  print(recall_score(y_test[i], y_preds, average='micro'))

for i in range(x_train.size):
  param_grid = {
      'C': [1.5, 1.6, 1.7],
      'kernel': ['linear', 'rbf', 'poly'],
      'gamma': [0.1, 1],
      'degree': [2, 3]
  }

  svc = SVC()
  grid_search_svc = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', verbose=2)

  grid_search_svc.fit(x_train[i], y_train[i])

  print("Best Parameters for SVC:", grid_search_svc.best_params_)
  print("Best Accuracy for SVC:", grid_search_svc.best_score_)

  best_model = grid_search_svc.best_estimator_
  y_preds = best_model.predict(x_test[i])

  print(accuracy_score(y_test[i], y_preds))
  print(f1_score(y_test[i], y_preds, average='micro'))
  print(precision_score(y_test[i], y_preds, average='micro'))
  print(recall_score(y_test[i], y_preds, average='micro'))

"""## Random Forest"""

for i in range(x_train):
  rf = RandomForestClassifier()

  rf.fit(x_train[i], y_train[i])
  y_preds = rf.predict(x_test[i])
  print(f1_score(y_test[i], y_preds, average='micro'))
  print(precision_score(y_test[i], y_preds, average='micro'))
  print(recall_score(y_test[i], y_preds, average='micro'))
  print(accuracy_score(y_test[i], y_preds))

for i in range(x_train):
  param_grid_rf = {
      'n_estimators': [100, 200, 300],
      'max_depth': [2, 4, 5],
      'min_samples_split': [2, 5],
      'min_samples_leaf': [1, 2],
  }

  rf = RandomForestClassifier(random_state=42)
  grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', verbose=2)


  grid_search_rf.fit(x_train[i], y_train[i])

  print("Best Parameters for Random Forest:", grid_search_rf.best_params_)
  print("Best Accuracy for Random Forest:", grid_search_rf.best_score_)

  best_model = grid_search_rf.best_estimator_
  y_preds = best_model.predict(x_test[i])

  print(accuracy_score(y_test[i], y_preds))
  print(f1_score(y_test[i], y_preds, average='micro'))
  print(precision_score(y_test[i], y_preds, average='micro'))
  print(recall_score(y_test[i], y_preds, average='micro'))

"""## XGBoost"""

for i in range(x_train):
  xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)
  xgb_param_grid = {
      'n_estimators': [100, 200, 300],
      'max_depth': [3, 5, 7],
      'learning_rate': [0.01, 0.1, 0.2],
      'subsample': [0.6, 0.8, 1.0],
      'colsample_bytree': [0.6, 0.8, 1.0],
      'reg_alpha': [0, 0.1, 1],
      'reg_lambda': [1, 1.5, 2]
  }

  xgb_grid_search = GridSearchCV(
      estimator=xgb_model,
      param_grid=xgb_param_grid,
      scoring='neg_mean_squared_error',
      cv=3,
      verbose=1,
      n_jobs=-1
  )

  xgb_grid_search.fit(x_train[i], y_train[i])
  print("Best XGBoost Parameters:", xgb_grid_search.best_params_)
  print("Best XGBoost Score:", -xgb_grid_search.best_score_)

  best_model = xgb_grid_search.best_estimator_
  y_preds = best_model.predict(x_test[i])

  print(accuracy_score(y_test[i], y_preds))
  print(f1_score(y_test[i], y_preds, average='micro'))
  print(precision_score(y_test[i], y_preds, average='micro'))
  print(recall_score(y_test[i], y_preds, average='micro'))

"""## LightGBM"""

for i in range(x_train):
  lgbm_model = LGBMRegressor(objective='regression', random_state=42)
  lgbm_param_grid = {
      'n_estimators': [100, 200, 300],
      'max_depth': [3, 5, 7, -1],
      'learning_rate': [0.01, 0.1, 0.2],
      'num_leaves': [20, 31, 40],
      'subsample': [0.6, 0.8, 1.0],
      'colsample_bytree': [0.6, 0.8, 1.0],
      'reg_alpha': [0, 0.1, 1],
      'reg_lambda': [1, 1.5, 2]
  }

  lgbm_random_search = RandomizedSearchCV(
      estimator=lgbm_model,
      param_distributions=lgbm_param_grid,
      scoring='neg_mean_squared_error',
      cv=3,
      n_iter=50,
      verbose=1,
      random_state=42,
      n_jobs=-1
  )

  lgbm_random_search.fit(x_train[i], y_train[i])
  print("Best LightGBM Parameters:", lgbm_random_search.best_params_)
  print("Best LightGBM Score:", -lgbm_random_search.best_score_)

  best_model = lgbm_random_search.best_estimator_
  y_preds = best_model.predict(x_test[i])

  print(accuracy_score(y_test[i], y_preds))
  print(f1_score(y_test[i], y_preds, average='micro'))
  print(precision_score(y_test[i], y_preds, average='micro'))
  print(recall_score(y_test[i], y_preds, average='micro'))